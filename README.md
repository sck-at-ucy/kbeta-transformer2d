[![CIÂ (macOSÂ arm64)](https://github.com/sck-at-ucy/kbeta/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/sck-at-ucy/kbeta/actions/workflows/ci.yml)

# kbetaÂ â€“Â *Kourkoutasâ€‘Î² Optimiser* Â Â ğŸŒğŸ¦ğŸš€ğŸ“ˆ

> **Research code for our upcoming paper
> â€œKourkoutasâ€‘Î²: Softâ€‘max Momentum with Adaptive Variance for Meshâ€‘Accelerated Deep Learning.â€**
> The repository ships the optimiser **plus two demonstration workloads** (a 2â€‘D dataâ€‘driven Transformer and a 3â€‘D PINN).

---

## Tableâ€¯ofâ€¯Contents
1. [Key ideas](#key-ideas)
2. [Project layout](#project-layout)
3. [Quick start](#quick-start)
4. [Using Kourkoutasâ€‘Î² in your own model](#minimal-example)
5. [Running the demo workloads](#demo-workloads)
6. [Tests & linting](#tests--linting)
7. [Citation](#citation)
8. [License](#license)
9. [Contributing & roadmap](#contributing--roadmap)

---

## Key ideas

* **Softâ€‘max variance tracking** to tame gradient spikes.
* **Two Î²â‚‚ parameters**:
  *Î²â‚‚_min* for ultraâ€‘fast warmâ€‘up, *Î²â‚‚_max* for longâ€‘term stability.
* **Layerâ€‘wise adaptive tinyâ€‘values** (Ïµ, spike dampers) that shrink with training progress.
* 100â€¯% **Appleâ€‘MLX** compatible â€“ no PyTorch required.

See detailed derivations in the forthcoming preâ€‘print (link will appear here).

---

## Conceptual overview

### Highâ€‘level intuition â€“ the â€œdesert lizardâ€ view
*Kourkoutasâ€‘Î²* is an Adamâ€‘style optimiser whose secondâ€‘moment decay **Î²â‚‚** is no longer a hardâ€‘wired constant.
Instead, every update computes a **sunâ€‘spike score**â€”a single, cheap scalar that compares the current gradient magnitude to its exponentiallyâ€‘weighted history.  We then **map that score to Î²â‚‚ on the fly**:

| Sunâ€‘spike | Lizard metaphor | Adaptive behaviour |
|-----------|-----------------|--------------------|
| **High**  | The desert sun is scorching â€” the lizard is â€œfully warmed upâ€ and sprints. | **Lower Î²â‚‚ toward Î²â‚‚,min** â†’ secondâ€‘moment memory shortens, allowing rapid, large parameter moves. |
| **Low**   | Itâ€™s cool; the lizard feels sluggish and takes cautious steps. | **Raise Î²â‚‚ toward Î²â‚‚,max** â†’ longer memory, filtering noise and producing steadier updates. |

Because the sunâ€‘spike diagnostic **exists only in Kourkoutasâ€‘Î²**, the method can be viewed as *Adam with a temperatureâ€‘controlled Î²â‚‚ schedule*: warm gradients trigger exploration; cooler gradients favour exploitation and stability.

---

## Project layout

```
kbeta
â”œâ”€â”€ src/kbeta/               # pip package
â”‚Â Â  â”œâ”€â”€ __init__.py          # reâ€‘exports optimiser
â”‚Â Â  â””â”€â”€ optim/
â”‚Â Â      â”œâ”€â”€ __init__.py
â”‚Â Â      â””â”€â”€ kbeta_softmax.py # <-- KourkoutasSoftmaxFlex implementation
â”‚
â”œâ”€â”€ workloads/
â”‚Â Â  â”œâ”€â”€ transformer/         # 2â€‘D heatâ€‘diffusion demo
â”‚Â Â  â””â”€â”€ pinn3d/              # 3â€‘D PINN demo
â”‚
â”œâ”€â”€ tests/                   # pytest suite (incl. smoke test)
â”œâ”€â”€ docs/                    # sphinx material (optional)
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md                # you are here
```

---

## Quick start

```bash
# 1. clone *your* fork (recommended) and cd into it
git clone git@github.com:<YOUR-USERNAME>/kbeta.git
cd kbeta

# 2. create a fresh virtualenv
python -m venv .venv && source .venv/bin/activate

# 3. editable install + dev extras
pip install -e ".[dev]"

# 4. run the ultraâ€‘short smoke test
pytest -q                       # should print â€˜1 passedâ€™
```

---

## Minimal example

```python
import mlx.core as mx
from kbeta.optim import KourkoutasSoftmaxFlex
import mlx.nn as nn

# dummy singleâ€‘parameter model
class Dummy(nn.Module):
    def __init__(self):
        super().__init__()
        self.w = mx.zeros((3,))

    def __call__(self, x):
        return (self.w * x).sum()

model = Dummy()
optim = KourkoutasSoftmaxFlex(learning_rate=1e-3)
optim.init(model.parameters())

x = mx.ones((3,))
loss, grads = nn.value_and_grad(model)(model, x)
optim.update(model, grads)  # â† one training step
```

---

## Demo workloads

| Folder | Paper section | What it shows | How to run |
|--------|---------------|---------------|------------|
| `workloads/transformer` | Â§â€¯4.1 | 2â€‘D heatâ€‘diffusion **dataâ€‘driven Transformer** trained with Kourkoutasâ€‘Î² vs Adam | `python -m transformer.Train --config configs/base.yaml` |
| `workloads/pinn3d` | Â§â€¯4.2 | 3â€‘D physicsâ€‘informed neural network (**PINN**) on a diffusion PDE | `python train_pinn3d.py --optimizer kourkoutas` |

All configs are pure YAML; commandâ€‘line `--override KEY=VAL` flags allow rapid sweeps.

---

## Tests & linting

```bash
pytest                 # unit tests
ruff check .           # style / import / naming
pre-commit run --all   # everything (if you installed the hooks)
```

Continuous Integration (CI) will refuse a PR that fails any of the above.

---

## Citation

```
@article{Kourkoutas2025,
  title   = {Kourkoutasâ€‘Î²: Softâ€‘max Momentum with Adaptive Variance},
  author  = {S. Kassinos and etÂ al.},
  journal = {ArXiv preprint},
  year    = {2025},
  url     = {https://arxiv.org/abs/XXXXX}
}
```

---

## License

This work is distributed under the **MIT License**â€”see [`LICENSE`](LICENSE) for details.

---

## Contributing & roadmap

We welcome issues & PRs!
Planned milestones:

1. **v0.1.0** â€“ optimiser + 2â€‘D Transformer demo (public).
2. **v0.2.0** â€“ 3â€‘D PINN demo, mixedâ€‘precision benchmarks.
3. **v1.0.0** â€“ journal paper release, pip wheels for macOS/Apple Silicon & Linux.

If you run into trouble, open an issue or ping `@stavrosâ€‘k` on GitHub.

Happy sprinting in the (numerical) desert ğŸŒğŸ¦ğŸš€ğŸ“ˆ
