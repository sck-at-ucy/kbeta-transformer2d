[![CIÂ (macOSÂ arm64)](https://github.com/sck-at-ucy/kbeta-transformer2d/actions/workflows/ci.yml/badge.svg)](https://github.com/sck-at-ucy/kbeta-transformer2d/actions/workflows/ci.yml)

<p align="center">
  <img src="assets/MLX_Kourkoutas.png" width="300"/>
    <img src="assets/t_2dframes.png" width="300"/>
</p>

# kbetaâ€‘transformer2dÂ â€“Â *2â€‘D Heatâ€‘Diffusion Transformer trained with Kourkoutasâ€‘Î²*Â Â ðŸŒžðŸ¦ŽðŸš€ðŸ“ˆ


> **Research companion code for the upcoming paper> â€œKourkoutasâ€‘Î²Â â€“ Softâ€‘max Momentum with Adaptive Variance for Meshâ€‘Accelerated Deepâ€¯Learning.â€**> > This repository contains the full **2â€‘D dataâ€‘driven Transformer** workload that accompanies the optimiser > (see the separate [`kbeta`](https://github.com/sck-at-ucy/kbeta) repo), plus lightweight utilities for training, > evaluation and visualisation.

---

## Tableâ€¯ofâ€¯Contents
1. [Why a 2â€‘D Transformer?](#why-a-2d-transformer)
2. [Model highlights](#model-highlights)
3. [Project layout](#project-layout)
4. [Quick start](#quick-start)
5. [Training from scratch](#training-from-scratch)
6. [Using your own datasets](#using-your-own-datasets)
7. [TestsÂ & linting](#tests--linting)
8. [Relation to Kourkoutasâ€‘Î²](#relation-to-kourkoutas-Î²)
9. [Citation](#citation)
10. [License](#license)

---

## Why a 2â€‘D Transformer?

* **Spatialâ€‘temporal diffusion** appears in countless engineering problems (heat flow, pollutant transport, â€¦).  
* A *purely dataâ€‘driven* Transformer offers a clean stressâ€‘test for the optimiser
* Solverâ€‘free physics loss: we embed the heatâ€‘equation residual as an analytic term, so no backâ€‘prop through external PDE solvers is requiredâ€”MLX autograd still handles all neuralâ€‘network gradients.â€”no PDE loss terms, no handâ€‘tuned schedulers.  
* The model scales to **512â€¯Ã—â€¯512 meshes on AppleÂ Silicon** while remaining <2â€¯M parameters; perfect for rapid experimentation.

---

## Model highlights               
*(whatâ€™s special about HeatDiffusionâ€‘Transformerâ€‘2D)*

* **Patchâ€‘wise attention on 2â€‘D grids**  
  The input tensor is reshaped into *(TÂ Ã—Â HÂ Ã—Â W)* patches, letting the model
  treat every spatial location symmetrically while still exploiting MXâ€‘GPU
  tensor cores efficiently.

* **Dual masking modes**  
  *Causal* masks give an autoregressive model useful for longâ€‘horizon rollout
  tests; *block* masks allow fullâ€‘context training when future frames are
  available.

* **RoPE (Rotary Positional Encoding)** in the **time** dimension  
  A single line swap lets you switch between vanilla sinusoidal encodings and
  RoPE, which markedly improves extrapolation beyond the training window.

* **Activation quantisation ready**  
  All dense / conv projections are implemented with `mlx.nn.quantize_lin`,
  giving you 8â€‘bit weights on Apple Silicon **without** code changes.

* **Learnable Fourier feature injection**  
  Two Fourier feature channels are concatenated to every patch embedding,
  stabilising training on very fine meshes.

* **Tiny footprint** â€“ 2.3â€¯M parameters  
  Fits comfortably on a single Mâ€‘series GPU core at batchâ€‘sizeÂ 32, even in
  FP16.

* **Oneâ€‘liner optimiser swap**  
  The model inherits its optimiser object, so comparing Adamâ€¯vsâ€¯Kourkoutasâ€‘Î²
  is literally *one* YAML entry.

---

## Project layout

```
kbeta-transformer2d
â”œâ”€â”€ src/kbeta_transformer2d/
â”‚Â Â  â”œâ”€â”€ __init__.py          # public API
â”‚Â Â  â”œâ”€â”€ data.py              # mesh generation + loaders
â”‚Â Â  â”œâ”€â”€ model.py             # Transformer & loss
â”‚Â Â  â”œâ”€â”€ optim_factory.py     # Kourkoutasâ€‘Î² wiring
â”‚Â Â  â”œâ”€â”€ train.py             # training / eval loops
â”‚Â Â  â”œâ”€â”€ plot_utils.py        # visualisations
â”‚Â Â  â””â”€â”€ demo_heat2d.py       # CLI entryâ€‘point
â”œâ”€â”€ configs/
â”‚Â Â  â””â”€â”€ heat2d.yml           # default hyperâ€‘params
â”œâ”€â”€ tests/                   # smoke tests
â””â”€â”€ README.md                # you are here
```

---

## Quick start

```bash
# 1) clone & set up a fresh virtualenv (AppleÂ Silicon, PythonÂ 3.11)
git clone https://github.com/sck-at-ucy/kbeta-transformer2d.git
cd kbeta-transformer2d
python -m venv .venv && source .venv/bin/activate

# 2) install this package *and* the private optimiser
pip install -e ".[dev]"                    # installs mlx, ruff, pytest, â€¦
pip install "kbeta @ git+https://github.com/sck-at-ucy/kbeta.git@main"

# 3) verify everything works
pytest -q                                  # âžœ 2 tests should pass
```

---

## Training from scratch

```bash
python -m kbeta_transformer2d.demo_heat2d         configs/heat2d.yml                         --override model_params.epochs=30
```

The YAML file controls **mesh size, model depth, batchâ€‘size, optimiser settings, plotting**, etc.  
Any key can be overridden on the CLI via `--override key.subkey=value`.

### Collected artefacts

* checkpoints every *n* epochs (`.npz`)
* optimiser state for restartability
* `.json` run config
* optional GIFs of predicted vsÂ truth heat maps

---

## Using your own datasets

1. Provide a NumPy array shaped `[t, ny, nx]` with your temperature fields.  
2. Adjust `geometry.nx, geometry.ny` in `configs/heat2d.yml`.  
3. Replace the `generate_datasets()` stub in **data.py** with your loader.

Thatâ€™s itâ€”no code changes in the model or training loop are required.

---

## TestsÂ & linting

```bash
pytest                 # unit + smoke tests
ruff check .           # style / quality gate
mypy src               # (optional) static typing
```

Our GitHub Action (macOSâ€‘14, MLX backâ€‘end) blocks any PR that fails the above.

---

## Relation to Kourkoutasâ€‘Î²

`kbeta_transformer2d` **does not reâ€‘implement the optimiser**; it *consumes* it:  
`optim_factory.py` instantiates `KourkoutasSoftmaxFlex` from the `kbeta` package and demonstrates adaptiveâ€‘Î²â‚‚ in a challenging, meshâ€‘based setting.  
If you only need the optimiser, install **`kbeta`**.  If you want a readyâ€‘made workload to benchmark against Adam, install **this** repo.

---

## Citation

```
@misc{Kassinos2025Transformer2D,
  title        = {Dataâ€‘Driven 2â€‘D Heatâ€‘Diffusion TransformerÂ â€“ Companion Code},
  author       = {StavrosÂ Kassinos and others},
  howpublished = {GitHub},
  year         = {2025},
  note         = {https://github.com/sck-at-ucy/kbeta-transformer2d}
}
```

Please also cite the main *Kourkoutasâ€‘Î²* paper.

---

## License

Released under the **MIT License**.  See [`LICENSE`](LICENSE) for the full text.

Happy experimentingÂ â€” and may your gradients be sunnyÂ ðŸŒžðŸ¦ŽðŸš€ðŸ“ˆ

