[![Test (dev install)](https://github.com/sck-at-ucy/kbeta-transformer2d/actions/workflows/ci.yml/badge.svg?branch=main&job=test-dev)](...)
[![Test (wheel install)](https://github.com/sck-at-ucy/kbeta-transformer2d/actions/workflows/ci.yml/badge.svg?branch=main&job=test-wheel)](...)

<p align="center">
  <img src="assets/MLX_Kourkoutas.png" width="300"/>
  <img src="assets/t_2dframes.png" width="300"/>
</p>

# kbeta-transformer2d â€“ *2-D Heat-Diffusion Transformer trained with Kourkoutas-Î²*  ðŸŒžðŸ¦ŽðŸš€ðŸ“ˆ

> **Research companion code for the upcoming paper**  
> â€œKourkoutasâ€‘Î²Â â€“ Softâ€‘max Momentum with Adaptive Variance for Meshâ€‘Accelerated Deepâ€¯Learning.â€  
> Published as [arXiv:2508.12996](http://arxiv.org/abs/2508.12996).
>
> This repository contains the full **2â€‘D dataâ€‘driven Transformer** workload that accompanies the optimiser  
> (see the separate [`kbeta`](https://github.com/sck-at-ucy/kbeta) repo), plus lightweight utilities for training,  
> evaluation and visualisation.

---

## Tableâ€¯ofâ€¯Contents
1. [Why a 2â€‘D Transformer?](#why-a-2d-transformer)
2. [Model highlights](#model-highlights)
3. [Project layout](#project-layout)
4. [Installation](#installation)
5. [Quick start](#quick-start)
6. [Commandâ€‘line interfaceÂ (CLI)](#command-line-interface-cli)
7. [Training from scratch](#training-from-scratch)
8. [Using your own datasets](#using-your-own-datasets)
9. [TestsÂ & linting](#tests--linting)
10. [Relation to Kourkoutasâ€‘Î²](#relation-to-kourkoutas-Î²)
11. [Citation](#citation)
12. [License](#license)

---

## Why a 2â€‘D Transformer?
* **Spatialâ€‘temporal diffusion** appears in countless engineering problems (heat flow, pollutant transport, â€¦).  
* A *purely dataâ€‘driven* Transformer offers a clean stressâ€‘test for the optimiser.  
* Solverâ€‘free physics loss: we embed the heatâ€‘equation residual as an analytic term, no backâ€‘prop through external PDE solvers is required.  
* The model scales to **512â€¯Ã—â€¯512 meshes on AppleÂ Silicon** while remaining <2â€¯M parameters; perfect for rapid experimentation.

---

## Model highlights
*(whatâ€™s special about HeatDiffusionâ€‘Transformerâ€‘2D)*

* **Patchâ€‘wise attention on 2â€‘D grids**  
  The input tensor is reshaped into *(TÂ Ã—Â HÂ Ã—Â W)* patches, letting the model treat every spatial location symmetrically while still exploiting MXâ€‘GPU tensor cores efficiently.

* **Dual masking modes**  
  *Causal* masks give an autoregressive model useful for longâ€‘horizon rollout tests; *block* masks allow fullâ€‘context training when future frames are available.

* **RoPE (Rotary Positional Encoding) in the time dimension**  
  A single line swap lets you switch between vanilla sinusoidal encodings and RoPE, which markedly improves extrapolation beyond the training window.

* **Activation quantisation ready**  
  All dense / conv projections are implemented with `mlx.nn.quantize_lin`, giving you 8â€‘bit weights on Apple Silicon **without** code changes.

* **Tiny footprint â€“ 2.3â€¯M parameters**  
  Fits comfortably on a single Mâ€‘series GPU core at batchâ€‘sizeÂ 32, even in FP16.

* **Oneâ€‘liner optimiser swap**  
  The model inherits its optimiser object, so comparing Adamâ€¯vsâ€¯Kourkoutasâ€‘Î² is literally *one* YAML entry.

---

## Project layout
```text
kbeta-transformer2d
â”œâ”€â”€ src/kbeta_transformer2d/
â”‚Â Â  â”œâ”€â”€ data.py              # mesh generation + loaders
â”‚Â Â  â”œâ”€â”€ model.py             # Transformer & loss
â”‚Â Â  â”œâ”€â”€ optim_factory.py     # Kourkoutasâ€‘Î² wiring
â”‚Â Â  â”œâ”€â”€ train.py             # training / eval loops
â”‚Â Â  â”œâ”€â”€ plot_utils.py        # visualisations
â”‚Â Â  â””â”€â”€ demo_heat2d.py       # CLI entryâ€‘point
â”œâ”€â”€ configs/
â”‚Â Â  â””â”€â”€ heat2d.yml           # default hyperâ€‘params
â”‚Â Â  â””â”€â”€ paper.yml            # paper   hyperâ€‘params
â””â”€â”€ README.md                # you are here
```

---

## Installation

### Option 1: PyPI wheels (endâ€‘users)
If you only want to run the Transformer benchmark with the latest `kbeta`:

```bash
pip install kbeta-transformer2d
```

For dev tools and tests:

```bash
pip install "kbeta-transformer2d[dev]"
```

For exact reproducibility of the paper (MLX 0.26.3 + pinned deps):

```bash
pip install "kbeta-transformer2d[repro]"
```

---

### Option 2: Cloning the repo (researchers / contributors)

```bash
git clone https://github.com/sck-at-ucy/kbeta-transformer2d.git
cd kbeta-transformer2d
python -m venv .venv && source .venv/bin/activate
pip install -e ".[dev]"
```

This makes all configs and scripts editable for research use.

---

## Quick start
```bash
pytest -q   # âžœ all smokeâ€‘tests should pass
```

---
