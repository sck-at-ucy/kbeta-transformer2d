# kbetaâ€‘transformer2dÂ â€“Â *2â€‘D Heatâ€‘Diffusion Transformer trained with Kourkoutasâ€‘Î²*Â Â ðŸŒžðŸ¦ŽðŸš€ðŸ“ˆ

[![CIÂ (macOSÂ arm64)](https://github.com/sck-at-ucy/kbeta-transformer2d/actions/workflows/ci.yml/badge.svg)](https://github.com/sck-at-ucy/kbeta-transformer2d/actions/workflows/ci.yml)

> **Research companion code for the upcoming paper> â€œKourkoutasâ€‘Î²Â â€“ Softâ€‘max Momentum with Adaptive Variance for Meshâ€‘Accelerated Deepâ€¯Learning.â€**> > This repository contains the full **2â€‘D dataâ€‘driven Transformer** workload that accompanies the optimiser > (see the separate [`kbeta`](https://github.com/sck-at-ucy/kbeta) repo), plus lightweight utilities for training, > evaluation and visualisation.

---

## Tableâ€¯ofâ€¯Contents
1. [Why a 2â€‘D Transformer?](#why-a-2d-transformer)
2. [Project layout](#project-layout)
3. [Quick start](#quick-start)
4. [Training from scratch](#training-from-scratch)
5. [Using your own datasets](#using-your-own-datasets)
6. [TestsÂ & linting](#tests--linting)
7. [Relation to Kourkoutasâ€‘Î²](#relation-to-kourkoutas-Î²)
8. [Citation](#citation)
9. [License](#license)

---

## Why a 2â€‘D Transformer?

* **Spatialâ€‘temporal diffusion** appears in countless engineering problems (heat flow, pollutant transport, â€¦).  
* A *purely dataâ€‘driven* Transformer offers a clean stressâ€‘test for the optimiserâ€”no PDE loss terms, no handâ€‘tuned schedulers.  
* The model scales to **512â€¯Ã—â€¯512 meshes on AppleÂ Silicon** while remaining <2â€¯M parameters; perfect for rapid experimentation.

---

## Project layout

```
kbeta-transformer2d
â”œâ”€â”€ src/kbeta_transformer2d/
â”‚Â Â  â”œâ”€â”€ __init__.py          # public API
â”‚Â Â  â”œâ”€â”€ data.py              # mesh generation + loaders
â”‚Â Â  â”œâ”€â”€ model.py             # Transformer & loss
â”‚Â Â  â”œâ”€â”€ optim_factory.py     # Kourkoutasâ€‘Î² wiring
â”‚Â Â  â”œâ”€â”€ train.py             # training / eval loops
â”‚Â Â  â”œâ”€â”€ plot_utils.py        # visualisations
â”‚Â Â  â””â”€â”€ demo_heat2d.py       # CLI entryâ€‘point
â”œâ”€â”€ configs/
â”‚Â Â  â””â”€â”€ heat2d.yml           # default hyperâ€‘params
â”œâ”€â”€ tests/                   # smoke tests
â””â”€â”€ README.md                # you are here
```

---

## Quick start

```bash
# 1) clone & set up a fresh virtualenv (AppleÂ Silicon, PythonÂ 3.11)
git clone https://github.com/sck-at-ucy/kbeta-transformer2d.git
cd kbeta-transformer2d
python -m venv .venv && source .venv/bin/activate

# 2) install this package *and* the private optimiser
pip install -e ".[dev]"                    # installs mlx, ruff, pytest, â€¦
pip install "kbeta @ git+https://github.com/sck-at-ucy/kbeta.git@main"

# 3) verify everything works
pytest -q                                  # âžœ 2 tests should pass
```

---

## Training from scratch

```bash
python -m kbeta_transformer2d.demo_heat2d         configs/heat2d.yml                         --override model_params.epochs=30
```

The YAML file controls **mesh size, model depth, batchâ€‘size, optimiser settings, plotting**, etc.  
Any key can be overridden on the CLI via `--override key.subkey=value`.

### Collected artefacts

* checkpoints every *n* epochs (`.npz`)
* optimiser state for restartability
* `.json` run config
* optional GIFs of predicted vsÂ truth heat maps

---

## Using your own datasets

1. Provide a NumPy array shaped `[t, ny, nx]` with your temperature fields.  
2. Adjust `geometry.nx, geometry.ny` in `configs/heat2d.yml`.  
3. Replace the `generate_datasets()` stub in **data.py** with your loader.

Thatâ€™s itâ€”no code changes in the model or training loop are required.

---

## TestsÂ & linting

```bash
pytest                 # unit + smoke tests
ruff check .           # style / quality gate
mypy src               # (optional) static typing
```

Our GitHub Action (macOSâ€‘14, MLX backâ€‘end) blocks any PR that fails the above.

---

## Relation to Kourkoutasâ€‘Î²

`kbeta_transformer2d` **does not reâ€‘implement the optimiser**; it *consumes* it:  
`optim_factory.py` instantiates `KourkoutasSoftmaxFlex` from the `kbeta` package and demonstrates adaptiveâ€‘Î²â‚‚ in a challenging, meshâ€‘based setting.  
If you only need the optimiser, install **`kbeta`**.  If you want a readyâ€‘made workload to benchmark against Adam, install **this** repo.

---

## Citation

```
@misc{Kassinos2025Transformer2D,
  title        = {Dataâ€‘Driven 2â€‘D Heatâ€‘Diffusion TransformerÂ â€“ Companion Code},
  author       = {StavrosÂ Kassinos and others},
  howpublished = {GitHub},
  year         = {2025},
  note         = {https://github.com/sck-at-ucy/kbeta-transformer2d}
}
```

Please also cite the main *Kourkoutasâ€‘Î²* paper.

---

## License

Released under the **MIT License**.  See [`LICENSE`](LICENSE) for the full text.

Happy experimentingÂ â€” and may your gradients be sunnyÂ ðŸŒžðŸ¦ŽðŸš€ðŸ“ˆ

