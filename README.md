[![CIÂ (macOSÂ arm64)](https://github.com/sck-at-ucy/kbeta-transformer2d/actions/workflows/ci.yml/badge.svg)](https://github.com/sck-at-ucy/kbeta-transformer2d/actions/workflows/ci.yml)

<p align="center">
  <img src="assets/MLX_Kourkoutas.png" width="300"/>
  <img src="assets/t_2dframes.png" width="300"/>
</p>

# kbetaâ€‘transformer2dÂ â€“Â *2â€‘D Heatâ€‘Diffusion Transformer trained with Kourkoutasâ€‘Î²*Â Â ğŸŒğŸ¦ğŸš€ğŸ“ˆ


> **Research companion code for the upcoming paper**  
> â€œKourkoutasâ€‘Î²Â â€“ Softâ€‘max Momentum with Adaptive Variance for Meshâ€‘Accelerated Deepâ€¯Learning.â€  
> This repository contains the full **2â€‘D dataâ€‘driven Transformer** workload that accompanies the optimiser  
> (see the separate [`kbeta`](https://github.com/sck-at-ucy/kbeta) repo), plus lightweight utilities for training,  
> evaluation and visualisation.

---

## Tableâ€¯ofâ€¯Contents
1. [Why a 2â€‘D Transformer?](#why-a-2d-transformer)
2. [Model highlights](#model-highlights)
3. [Project layout](#project-layout)
4. [Quick start](#quick-start)
5. [Commandâ€‘line interfaceÂ (CLI)](#commandâ€‘line-interface-cli)
6. [Training from scratch](#training-from-scratch)
7. [Using your own datasets](#using-your-own-datasets)
8. [TestsÂ & linting](#tests--linting)
9. [Relation to Kourkoutasâ€‘Î²](#relation-to-kourkoutas-Î²)
10. [Citation](#citation)
11. [License](#license)

---

## Why a 2â€‘D Transformer?
* **Spatialâ€‘temporal diffusion** appears in countless engineering problems (heat flow, pollutant transport, â€¦).  
* A *purely dataâ€‘driven* Transformer offers a clean stressâ€‘test for the optimiser.  
* Solverâ€‘free physics loss: we embed the heatâ€‘equation residual as an analytic term, no backâ€‘prop through external PDE solvers is required.  
* The model scales to **512â€¯Ã—â€¯512 meshes on AppleÂ Silicon** while remaining &lt;2â€¯M parameters; perfect for rapid experimentation.

---

## Model highlights
*(whatâ€™s special about HeatDiffusionâ€‘Transformerâ€‘2D)*

* **Patchâ€‘wise attention on 2â€‘D grids**  
  The input tensor is reshaped into *(TÂ Ã—Â HÂ Ã—Â W)* patches, letting the model treat every spatial location symmetrically while still exploiting MXâ€‘GPU tensor cores efficiently.

* **Dual masking modes**  
  *Causal* masks give an autoregressive model useful for longâ€‘horizon rollout tests; *block* masks allow fullâ€‘context training when future frames are available.

* **RoPE (Rotary Positional Encoding) in the time dimension**  
  A single line swap lets you switch between vanilla sinusoidal encodings and RoPE, which markedly improves extrapolation beyond the training window.

* **Activation quantisation ready**  
  All dense / conv projections are implemented with `mlx.nn.quantize_lin`, giving you 8â€‘bit weights on Apple Silicon **without** code changes.

* **Tiny footprint â€“ 2.3â€¯M parameters**  
  Fits comfortably on a single Mâ€‘series GPU core at batchâ€‘sizeÂ 32, even in FP16.

* **Oneâ€‘liner optimiser swap**  
  The model inherits its optimiser object, so comparing Adamâ€¯vsâ€¯Kourkoutasâ€‘Î² is literally *one* YAML entry.

---

## Project layout
```text
kbeta-transformer2d
â”œâ”€â”€ src/kbeta_transformer2d/
â”‚Â Â  â”œâ”€â”€ data.py              # mesh generation + loaders
â”‚Â Â  â”œâ”€â”€ model.py             # Transformer & loss
â”‚Â Â  â”œâ”€â”€ optim_factory.py     # Kourkoutasâ€‘Î² wiring
â”‚Â Â  â”œâ”€â”€ train.py             # training / eval loops
â”‚Â Â  â”œâ”€â”€ plot_utils.py        # visualisations
â”‚Â Â  â””â”€â”€ demo_heat2d.py       # CLI entryâ€‘point
â”œâ”€â”€ configs/
â”‚Â Â  â””â”€â”€ heat2d.yml           # default hyperâ€‘params
â””â”€â”€ README.md                # you are here
```

---

## Quick start
```bash
# clone & set up a fresh virtualâ€‘env (AppleÂ Silicon, PythonÂ 3.11)
git clone https://github.com/sck-at-ucy/kbeta-transformer2d.git
cd kbeta-transformer2d
python -m venv .venv && source .venv/bin/activate

# install this repo (editable) + dev extras
pip install -e '.[dev]'
# install optimiser (private for now)
pip install 'kbeta @ git+https://github.com/sck-at-ucy/kbeta.git@main'

pytest -q   # âœ all smokeâ€‘tests should pass
```

---

## Commandâ€‘line interfaceÂ (CLI)

`demo_heat2d.py` is both importâ€‘able **and** executable:

```bash
python -m kbeta_transformer2d.demo_heat2d <CONFIG.yml> [flags]
```

| element           | purpose                                                                                                    |
|-------------------|-------------------------------------------------------------------------------------------------------------|
| **CONFIG.yml**    | Path to a YAML file. If *relative* and not found, it is resolved inside the *installed* package (`â€¦/configs`). |
| `--epochs N`      | shorthandâ€ƒâ†’ `model_params.epochs=N`                                                                         |
| `--seed N`        | RNG seed (NumPyâ€¯+â€¯MLX)                                                                                       |
| `--optimizer NAME`| `adam95`Â \|Â `adam999`Â \|Â `kourkoutas`                                                                       |
| `--kour_diagnostics` | turn on verbose internal stats in Kourkoutasâ€‘Î²                                                            |
| `--collect_spikes`   | store perâ€‘epoch spike stats for violin / density plots                                                    |
| `--viz`              | enable matplotlib previews                                                                                |
| `--override KEY=VAL [KEY=VALâ€¦]` | *generic* overrides using dotâ€‘notation. May be repeated.                                     |

### Examples
```bash
# train 5Â epochs with vanilla Adamâ€‘(0.9,0.95)
python -m kbeta_transformer2d.demo_heat2d heat2d.yml --epochs=5 --optimizer=adam95

# same as above but change mesh size and disable plotting
python -m kbeta_transformer2d.demo_heat2d heat2d.yml         --override geometry.dx=0.002 geometry.dy=0.002 viz.enabled=false

# run with the *packaged* default config (no file in cwd needed)
python - <<'PY'
import subprocess, importlib.resources as res
cfg = res.files('kbeta_transformer2d.configs') / 'heat2d.yml'
subprocess.run(['python','-m','kbeta_transformer2d.demo_heat2d', str(cfg), '--epochs=1'])
PY
```

### Default configuration (excerpt)
```yaml
seed: 30
geometry:
  rod_length: 0.05       # [m]
  rod_width:  0.05
  dx: 0.002
  dy: 0.002
model_params:
  time_steps: 1200
  num_heads: 8
  num_encoder_layers: 4
  mlp_dim: 1024
  embed_dim: 512
  batch_size: 32
  mask_type: causal      # (causal | block)
optimizer:
  name: adam999
  init_lr: 1.0e-3
  target_lr: 1.0e-5
  ramp_steps: 60000
```
*(see `configs/heat2d.yml` for the full list)*

---
### YAML quickâ€‘referenceÂ â€” common pitfalls ğŸ”

| what you want         | **write it like this**         | ğŸ‘€Â why it matters                                                |
|-----------------------|--------------------------------|-----------------------------------------------------------------|
| **Booleans**          | `true`, `false`Â (â€˜yesâ€™/â€˜noâ€™ are fine too) | YAML also treats `on`, `off`, `y`, `n` as booleans ğŸ‘€. Avoid surprises by sticking to `true`/`false`. |
| **Disable a feature** | `some_flag: false` **not** `0` | `0` parses as an *integer*, not a boolean.                      |
| **Integers**          | `epochs: 100`                  | No quotes â€‘â€‘ unless you *really* need a string.                  |
| **Floats**            | `lr: 1e-3` Â orÂ  `0.001`        | Scientific notation is fineÂ â€“ YAML keeps full precision.         |
| **Avoid octal traps** | `mode: "0755"` (quotes!)       | Bare `0755` is parsed as **octal** â†’ â€‘493 in Python.             |
| **Explicit null / off** | `momentum: null`Â (or `~`)     | Empty value **isnâ€™t** the same as `0`. Use `null` when you mean â€œunsetâ€. |
| **Lists**             |                                  | ```yml<br>betas: [0.9, 0.999]<br># or the long form<br>betas:<br>  - 0.9<br>  - 0.999``` |
| **Strings that look like numbers** | `activation: "gelu"`  | Quotes stop YAML from trying to coerce things like `"1e6"` into floats. |
| **Envâ€‘vars / paths**  | `data_dir: "${HOME}/datasets"` | The braces/`$` need **quotes** or theyâ€™ll be treated as plain text and lose the `$`. |
| **Indentation**       | Two spaces per level (never tabs) | YAML is indentationâ€‘sensitiveâ€”tabs are a syntax error.           |

> **Tip:** If youâ€™re ever unsure how YAML will parse a value, run  
> `python -c 'import yaml, sys, pprint, pathlib; pprint.pprint(yaml.safe_load(pathlib.Path("your.yml").read_text()))'`  
> to see exactly what Python receives.
---

## Training from scratch
```bash
python -m kbeta_transformer2d.demo_heat2d configs/heat2d.yml --epochs=30
```
Checkpoints (`.pkl` + `.safetensors`) and plots are written to **`src/kbeta_transformer2d/OUTPUTS/`** by default.

---

## Using your own datasets
1. Provide your dataset as NumPy/MLX arrays.  
2. Adjust `geometry.*` in the YAML to match mesh resolution.  
3. Replace `generate_datasets()` in `data.py`.

---

## TestsÂ & linting
```bash
pytest
ruff check .
mypy src
```

---

## Relation to Kourkoutasâ€‘Î²
This repo **uses** the optimiser from `kbeta`; it does *not* reâ€‘implement it.  
`optim_factory.py` wires `KourkoutasSoftmaxFlex` into the training loop.

---
### Further ReadingÂ & Related Resources ğŸ“š

| Resource | Why it Matters for **Kourkoutasâ€‘Î²** &Â **kbetaâ€‘transformer2d** |
|----------|--------------------------------------------------------------|
| **MLXâ€¯Beyondâ€¯Language (repo)**<br>https://github.com/sck-at-ucy/MLX_BeyondLanguage | Companion project that demonstrates how to scale MLX Transformer workloads *beyond* conventional languageâ€‘model settings (e.g. vision & physics). Provides many of the coding conventions, dataset helpers and plotting utilities reused here. |
| **MLXÂ framework (Apple)**<br>https://github.com/ml-explore/mlx | The underlying tensor/NN library that powers both Kourkoutasâ€‘Î² *and* the 2â€‘D Transformer. Understanding MLXâ€™s compile/runtime model explains why adaptive optimisers like Kourkoutasâ€‘Î² can hit full Metal GPU speed without custom CUDA kernels. |
| **Article: *Softâ€‘max Momentum with Adaptive Varianceâ€¦***<br>https://www.sciencedirect.com/science/article/pii/S2590123025009478 | The forthcoming paper describing Kourkoutasâ€‘Î² in detailâ€”mathematical derivation, convergence proofs and ablation studies. Read this to see why Î²â‚‚ must be a dynamic *distribution* rather than a constantÂ 0.999. |
| **kbeta (core optimiser)**<br>https://github.com/sck-at-ucy/kbeta | Standâ€‘alone Python package implementing Kourkoutasâ€‘Î². `kbeta_transformer2d` imports `KourkoutasSoftmaxFlex` from *this* repo; all optimiserâ€‘level issues/PRs belong there. |
| **kbetaâ€‘pinn3d (PINN benchmark)**<br>https://github.com/sck-at-ucy/kbeta-pinn3d | 3â€‘D Physicsâ€‘Informed Neural Network (PINN) workload that **collects Î²â‚‚ â€œspikeâ€ diagnostics** during training. Useful if you want to compare how Kourkoutasâ€‘Î² behaves on PDEâ€‘constrained training vs. the fully dataâ€‘driven 2â€‘D Transformer shown here. |
---

## Citation
```bibtex
@misc{Kassinos2025Transformer2D,
  title        = {Dataâ€‘Driven 2â€‘D Heatâ€‘Diffusion TransformerÂ â€“ Companion Code},
  author       = {StavrosÂ Kassinos and collaborators},
  year         = {2025},
  howpublished = {GitHub},
  note         = {\url{https://github.com/sck-at-ucy/kbeta-transformer2d}}
}
```

---

## License
MIT.  See [`LICENSE`](LICENSE) for the full text.

Happy experimentingÂ â€”Â and may your gradients be sunnyÂ â˜€ï¸ğŸ¦ğŸš€ğŸ“ˆ
